------------------------------------------------------------------------------------------------------------------------
LIST OF PAPERS
------------------------------------------------------------------------------------------------------------------------

@inproceedings{castellana2024lying,
    abbr = {Conference},
    title = {Lying Graph Convolution: Learning to Lie for Node Classification Tasks},
    author = {Castellana, Daniele},
    booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},
    note = {Accepted},
    year = {2024},
    abstract = {In the context of machine learning for graphs, many researchers have empirically observed that Deep Graph Networks (DGNs) perform favourably on node classification tasks when the graph structure is homophilic (\ie adjacent nodes are similar). In this paper, we introduce Lying-GCN, a new DGN inspired by opinion dynamics that can adaptively work in both the heterophilic and the homophilic setting. At each layer, each agent (node) shares its own opinions (node embeddings) with its neighbours. Instead of sharing its opinion directly as in GCN, we introduce a mechanism which allows agents to lie. Such a mechanism is adaptive, thus the agents learn how and when to lie according to the task that should be solved. We provide a characterisation of our proposal in terms of dynamical systems, by studying the spectral property of the coefficient matrix of the system. While the steady state of the system collapses to zero, we believe the lying mechanism is still usable to solve node classification tasks. We empirically prove our belief on both synthetic and real-world datasets, by showing that the lying mechanism allows to increase the performances in the heterophilic setting without harming the results in the homophilic one.},
    arxiv = {https://arxiv.org/pdf/2405.01247},
    selected = {true}
}

@inproceedings{castellana2023investigating,
    abbr = {Workshop},
    title = {Investigating the Interplay between Features and Structures in Graph Learning},
    author = {Castellana, Daniele and Errica, Federico},
    booktitle = {20th International Workshop on Mining and Learning with Graphs},
    year = {2023},
    pdf = {https://mlg-europe.github.io/2023/papers/219.pdf},
    abstract = {In the past, the dichotomy between homophily and heterophily has inspired research contributions toward a better understanding of Deep Graph Networks' inductive bias. In particular, it was believed that homophily strongly correlates with better node classification predictions of message-passing methods. More recently, however, researchers pointed out that such dichotomy is too simplistic as we can construct node classification tasks where graphs are completely heterophilic but the performances remain high. Most of these works have also proposed new quantitative metrics to understand when a graph structure is useful, which implicitly or explicitly assume the correlation between node features and target labels. Our work empirically investigates what happens when this strong assumption does not hold, by formalising two generative processes for node classification tasks that allow us to build and study ad-hoc problems. To quantitatively measure the influence of the node features on the target labels, we also use a metric we call Feature Informativeness. We construct six synthetic tasks and evaluate the performance of six models, including structure-agnostic ones. Our findings reveal that previously defined metrics are not adequate when we relax the above assumption. Our contribution to the workshop aims at presenting novel research findings that could help advance our understanding of the field.},
    selected = {true}
}

@inproceedings{castellana2024cd,
    abbr = {Conference},
    title = {CD-IMM: The Benefits of Domain-based Mixture Models in Bayesian Continual Learning},
    author = {Castellana, Daniele and Carta, Antonio and Bacciu, Davide},
    booktitle = {First ContinualAI Unconference-Preregistration Track},
    year = {2024},
    pdf = {https://openreview.net/pdf?id=AEQ3ObpfEG},
    url = {https://openreview.net/forum?id=AEQ3ObpfEG},
    abstract = {Real-world streams of data are characterised by the continuous occurrence of new and old classes, possibly on novel domains. Bayesian non-parametric mixture models provide a natural solution for continual learning due to their ability to create new components on the fly when new data are observed. However, popular class-based and time-based mixtures are often tested on simplified streams (\eg class-incremental), where shortcuts can be exploited to infer drifts. We hypothesise that \emph{domain-based mixtures are more effective on natural streams}. Our proposed method, the CD-IMM, exemplifies this approach by learning an infinite mixture of domains for each class. We experiment on a natural scenario with a mix of class repetitions and novel domains to validate our hypothesis.},
    selected = {true}
}

@InProceedings{castellana22,
    abbr = {Conference},
    title = {The Infinite Contextual Graph {M}arkov Model},
    author = {Castellana, Daniele and Errica, Federico and Bacciu, Davide and Micheli, Alessio},
    booktitle = {Proceedings of the 39th International Conference on Machine Learning},
    pages = {2721--2737},
    year = {2022},
    volume = {162},
    series = {Proceedings of Machine Learning Research},
    month = {17--23 Jul},
    publisher = {PMLR},
    pdf = {https://proceedings.mlr.press/v162/castellana22a/castellana22a.pdf},
    url = {https://proceedings.mlr.press/v162/castellana22a.html},
    abstract = {The Contextual Graph Markov Model (CGMM) is a deep, unsupervised, and probabilistic model for graphs that is trained incrementally on a layer-by-layer basis. As with most Deep Graph Networks, an inherent limitation is the need to perform an extensive model selection to choose the proper size of each layer’s latent representation. In this paper, we address this problem by introducing the Infinite Contextual Graph Markov Model (iCGMM), the first deep Bayesian nonparametric model for graph learning. During training, iCGMM can adapt the complexity of each layer to better fit the underlying data distribution. On 8 graph classification tasks, we show that iCGMM: i) successfully recovers or improves CGMM’s performances while reducing the hyper-parameters’ search space; ii) performs comparably to most end-to-end supervised methods. The results include studies on the importance of depth, hyper-parameters, and compression of the graph embeddings. We also introduce a novel approximated inference procedure that better deals with larger graph topologies.},
    selected = {true}
}


@phdthesis{Castellana2021PhD,
    abbr = {Thesis},
    title = {A tensor framework for learning in structured domains},
    author = {Castellana, Daniele},
    school = {Department of Computer Science, Università di Pisa},
    year = {2021},
    month = {may}
}

@inproceedings{Castellana2020esann,
    abbr = {Conference},
    abstract = {The paper introduces two new aggregation functions to encode structural knowledge from tree-structured data. They leverage the Canonical and Tensor-Train decompositions to yield expressive context aggregation while limiting the number of model parameters. Finally, we define two novel neural recursive models for trees leveraging such aggrega-tion functions, and we test them on two tree classification tasks, showing the advantage of proposed models when tree outdegree increases.},
    author = {Castellana, Daniele and Bacciu, Davide},
    booktitle = {Proceedings of the the 28th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
    isbn = {978-2-87587-074-2},
    keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
    title = {{Tensor Decompositions in Recursive Neural Networks for Tree-Structured Data}},
    year = {2020},
    month = {oct},
    pages = {451--456},
}

@inproceedings{Bacciu2018b,
    abbr = {Conference},
    abstract = {{\textcopyright} ESANN 2018 - Proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. The paper introduces a new probabilistic tree encoder based on a mixture of Bottom-up Hidden Tree Markov Models. The ability to recognise similar structures in data is experimentally assessed both in clusterization and classification tasks. The results of these preliminary experiments suggest that the model can be successfully used to compress the tree structural and label patterns in a vectorial representation.},
    author = {Bacciu, Davide and Castellana, Daniele},
    booktitle = {Proceedings of the 26th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
    isbn = {978-287587047-6},
    title = {{Mixture of Hidden Markov Models as tree encoder}},
    year = {2018},
    month = {apr},
    pages = {543--548}
}

@inproceedings{Castellana2019b,
    abbr = {Conference},
    abstract = {{\textcopyright} 2019 IEEE. Bottom-Up Hidden Tree Markov Model is a highly expressive model for tree-structured data. Unfortunately, it cannot be used in practice due to the intractable size of its state-transition matrix. We propose a new approximation which lies on the Tucker factorisation of tensors. The probabilistic interpretation of such approximation allows us to define a new probabilistic model for tree-structured data. Hence, we define the new approximated model and we derive its learning algorithm. Then, we empirically assess the effective power of the new model evaluating it on two different tasks. In both cases, our model outperforms the other approximated model known in the literature.},
    author = {Castellana, Daniele and Bacciu, Davide},
    booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
    doi = {10.1109/IJCNN.2019.8851851},
    isbn = {978-1-7281-1985-4},
    month = {jul},
    pages = {1--8},
    publisher = {IEEE},
    title = {{Bayesian Tensor Factorisation for Bottom-up Hidden Tree Markov Models}},
    url = {https://ieeexplore.ieee.org/document/8851851/},
    volume = {2019-July},
    year = {2019}
}

@inproceedings{bacciu2018learning,
    abbr = {Workshop},
    author = {Bacciu, Davide and Castellana, Daniele},
    booktitle = {Workshop on Learning and Automata (LearnAut'18)},
    title = {{Learning Tree Distributions by Hidden Markov Models}},
    year = {2018},
    month = {jul},
    keywords = {workshops}
}

@article{Bacciu2019a,
    abbr = {Journal},
    title = {Bayesian mixtures of Hidden Tree Markov Models for structured data clustering},
    journal = {Neurocomputing},
    volume = {342},
    pages = {49-59},
    year = {2019},
    note = {Advances in artificial neural networks, machine learning and computational intelligence},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2018.11.091},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231219301444},
    author = {Davide Bacciu and Daniele Castellana},
    keywords = {Hidden Tree Markov Models, Infinite mixtures, Dirichlet Process, Tree structured data},
    abstract = {The paper deals with the problem of unsupervised learning with structured data, proposing a mixture model approach to cluster tree samples. First, we discuss how to use the Switching-Parent Hidden Tree Markov Model, a compositional model for learning tree distributions, to define a finite mixture model where the number of components is fixed by a hyperparameter. Then, we show how to relax such an assumption by introducing a Bayesian non-parametric mixture model where the number of necessary hidden tree components is learned from data. Experimental validation on synthetic and real datasets show the benefit of mixture models over simple hidden tree models in clustering applications. Further, we provide a characterization of the behaviour of the two mixture models for different choices of their hyperparameters.}
}

@article{Castellana2021neurocomp,
    abbr = {Journal},
    selected = {true},
    title = {A tensor framework for learning in structured domains},
    journal = {Neurocomputing},
    year = {2021},
    issn = {0925-2312},
    doi = {https://doi.org/10.1016/j.neucom.2021.05.110},
    url = {https://www.sciencedirect.com/science/article/pii/S0925231221011164},
    author = {Daniele Castellana and Davide Bacciu},
    keywords = {Tensor decompositions, Structured data, Recursive neural models, Probabilistic models},
    abstract = {Learning machines for structured data (e.g., trees) are intrinsically based on their capacity to learn representations by aggregating information from the multi-way relationships emerging from the structure topology. While complex aggregation functions are desirable in this context to increase the expressiveness of the learned representations, the modelling of higher-order interactions among structure constituents is unfeasible, in practice, due to the exponential number of parameters required. Therefore, the common approach is to define models which rely only on first-order interactions among structure constituents. In this work, we leverage tensors theory to define a framework for learning in structured domains. Such a framework is built on the observation that more expressive models require a tensor parameterisation. This observation is the stepping stone for the application of tensor decompositions in the context of recursive models. From this point of view, the advantage of using tensor decompositions is twofold since it allows limiting the number of model parameters while injecting inductive biases that do not ignore higher-order interactions. We apply the proposed framework on probabilistic and neural models for structured data, defining different models which leverage tensor decompositions. The experimental validation clearly shows the advantage of these models compared to first-order and full-tensorial models.}
}

@inproceedings{Castellana2020coling,
    abbr = {Conference},
    title = {Learning from Non-Binary Constituency Trees via Tensor Decomposition},
    author = {Castellana, Daniele and Bacciu, Davide},
    booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
    month = {dec},
    year = {2020},
    publisher = {International Committee on Computational Linguistics},
    url = {https://aclanthology.org/2020.coling-main.346},
    doi = {10.18653/v1/2020.coling-main.346},
    pages = {3899--3910},
    abstract = {Processing sentence constituency trees in binarised form is a common and popular approach in literature. However, constituency trees are non-binary by nature. The binarisation procedure changes deeply the structure, furthering constituents that instead are close. In this work, we introduce a new approach to deal with non-binary constituency trees which leverages tensor-based models. In particular, we show how a powerful composition function based on the canonical tensor decomposition can exploit such a rich structure. A key point of our approach is the weight sharing constraint imposed on the factor matrices, which allows limiting the number of model parameters. Finally, we introduce a Tree-LSTM model which takes advantage of this composition function and we experimentally assess its performance on different NLP tasks.},
    keywords = {poster},
    selected = {true}
}

@inproceedings{Castellana2020ijcnn,
    abbr = {Conference},
    abstract = {Most machine learning models for structured data encode the structural knowledge of a node by leveraging simple aggregation functions (in neural models, typically a weighted sum) of the information in the node's neighbourhood. Nevertheless, the choice of simple context aggregation functions, such as the sum, can be widely sub-optimal. In this work we introduce a general approach to model aggregation of structural context leveraging a tensor-based formulation. We show how the exponential growth in the size of the parameter space can be controlled through an approximation based on the Tucker tensor decomposition. This approximation allows limiting the parameters space size, decoupling it from its strict relation with the size of the hidden encoding space. By this means, we can effectively regulate the trade-off between expressivity of the encoding, controlled by the hidden size, computational complexity and model generalisation, influenced by parameterisation. Finally, we introduce a new Tensorial Tree-LSTM derived as an instance of our framework and we use it to experimentally assess our working hypotheses on tree classification scenarios.},
    archivePrefix = {arXiv},
    arxivId = {2006.10021},
    author = {Castellana, Daniele and Bacciu, Davide},
    booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
    doi = {10.1109/IJCNN48605.2020.9206597},
    eprint = {2006.10021},
    isbn = {978-1-7281-6926-2},
    month = {jul},
    pages = {1--8},
    publisher = {IEEE},
    title = {{Generalising Recursive Neural Models by Tensor Decomposition}},
    url = {https://ieeexplore.ieee.org/document/9206597/},
    year = {2020}
}

------------------------------------------------------------------------------------------------------------------------
LIST OF TALKS
------------------------------------------------------------------------------------------------------------------------

@misc{trento2022,
    abbr = {Invited},
    title = {A Tensor Framework for Learning in Structured Domains },
    author = {Castellana, Daniele},
    year = {2022},
    month = {Apr},
    event = {Mathematics for Data Science, Artificial Intelligence and Machine Learning},
    location = {Department of Mathematics, University of Trento},
    url = {http://datascience.maths.unitn.it/events/Math4DS-AI-ML/index.html}
}


@misc{itis2023,
    abbr = {Invited},
    title = {Machine Learning: cosa c'è sotto?},
    author = {Castellana, Daniele},
    year = {2023},
    month = {Apr},
    event = {Giornata della Scienza},
    location = {I.I.S.S. Luigi Dell'Erba, Castellana Grotte (Bari)}
}
